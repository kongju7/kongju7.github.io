---
layout: post
title: "[LLM] Foundation Models 논문 모음"
subtitle: "쏟아지는 기반 모델 관련 논문들"
categories: Data_Science 
tags: [Data Science, NLP, LLM, Large Language Model, Foundation Models, paper]
---

## 대용량 언어 모델(LLM) 
## 기반 모델(Foundation Models) 관련 논문 모음 
  
  
### 1. 기반 모델 
  
**기반 모델 및 응용 분야**
- [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf){:target="_blank"}  
- [Multimodal Foundation Models: From Specialists to General-Purpose Assistants](https://arxiv.org/pdf/2309.10020.pdf){:target="_blank"}  
- [Large Multimodal Models: Notes on CVPR 2023 Tutorial](https://arxiv.org/pdf/2306.14895.pdf){:target="_blank"}  
- [Towards Generalist Biomedical AI](https://arxiv.org/pdf/2307.14334.pdf){:target="_blank"}  
- [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT](https://arxiv.org/pdf/2302.09419.pdf){:target="_blank"}  
- [Interactive Natural Language Processing](https://arxiv.org/pdf/2305.13246.pdf){:target="_blank"}  
- [Towards Reasoning in Large Language Models: A Survey](https://arxiv.org/pdf/2212.10403.pdf){:target="_blank"}  
  
  
**순환 신경망(RNN) 및 합성곱 신경망(CNN)**
- [Recurrent Neural Networks (RNNs): A gentle Introduction and Overview](https://arxiv.org/pdf/1912.05911.pdf){:target="_blank"}  
- [Highway Networks](https://arxiv.org/pdf/1505.00387.pdf){:target="_blank"}  
- [Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf){:target="_blank"}  
- [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/pdf/1412.3555.pdf){:target="_blank"}  
- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025.pdf){:target="_blank"}  
- [An Introduction to Convolutional Neural Networks](https://arxiv.org/pdf/1511.08458.pdf){:target="_blank"}  
- [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf){:target="_blank"}  
- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf){:target="_blank"}  
- [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf){:target="_blank"}  
- [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf){:target="_blank"}  
- [Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/pdf/1611.05431.pdf){:target="_blank"}  
- [A ConvNet for the 2020s](https://arxiv.org/pdf/2201.03545.pdf){:target="_blank"}  
  
  
**자연어 처리(NLP) 및 컴퓨터 비전(CV)**        				
- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf){:target="_blank"}  
- [Thumbs up? Sentiment Classification using Machine Learning Techniques](https://aclanthology.org/W02-1011.pdf){:target="_blank"}  
- [A Survey of Named Entity Recognition and Classification](https://nlp.cs.nyu.edu/sekine/papers/li07.pdf){:target="_blank"}  
- [Teaching Machines to Read and Comprehend](https://arxiv.org/pdf/1506.03340.pdf){:target="_blank"}  
- [Deep Neural Networks for Acoustic Modeling in Speech Recognition](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6296526	){:target="_blank"}  
- [A Neural Attention Model for Sentence Summarization](https://arxiv.org/pdf/1509.00685.pdf){:target="_blank"}  
- [Microsoft COCO: Common Objects in Context](https://arxiv.org/pdf/1405.0312.pdf){:target="_blank"}  
- [Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation](https://arxiv.org/pdf/1311.2524.pdf){:target="_blank"}  
- [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/pdf/1411.4038.pdf){:target="_blank"}  
- [DeepFace: Closing the Gap to Human-Level Performance in Face Verification](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf){:target="_blank"}  
- [DeepPose: Human Pose Estimation via Deep Neural Networks](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6909610	){:target="_blank"}  
- [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434v2.pdf){:target="_blank"}  
  
  
### 2. 트랜스포머(Transformers) 아키텍쳐  
  
**셀프 어텐션(Self Attention) 및 트랜스포머(Transformers)**
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf){:target="_blank"}  
- [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://proceedings.mlr.press/v37/xuc15.pdf){:target="_blank"}  
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf){:target="_blank"}  
- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/	){:target="_blank"}  
- [Image Transformer](https://arxiv.org/pdf/1802.05751.pdf){:target="_blank"}  
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://openreview.net/pdf?id=YicbFdNTTy	){:target="_blank"}  
  
  
**효율적인 트랜스포머**
- [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/pdf/2006.16236.pdf){:target="_blank"}  
- [Perceiver: General Perception with Iterative Attention](https://arxiv.org/pdf/2103.03206.pdf){:target="_blank"}  
- [Random Feature Attention](https://arxiv.org/pdf/2103.02143.pdf){:target="_blank"}  
- [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150.pdf){:target="_blank"}  
- [Generating Long Sequences with Sparse Transformers](https://arxiv.org/pdf/1904.10509.pdf){:target="_blank"}  
- [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/pdf/2006.04768.pdf){:target="_blank"}  
- [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/pdf/2111.00396.pdf){:target="_blank"}  
  
  
**매개변수 효율적 튜닝**
- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf){:target="_blank"}  
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf){:target="_blank"}  
- [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf){:target="_blank"}  
- [It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://arxiv.org/pdf/2009.07118.pdf){:target="_blank"}  
- [Making Pre-trained Language Models Better Few-shot Learners](https://aclanthology.org/2021.acl-long.295.pdf){:target="_blank"}
- [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/pdf/2205.05638.pdf){:target="_blank"}  
- [Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/pdf/2110.04366.pdf){:target="_blank"}  

**언어 모델 사전 학습**
- [Deep Contextualized Word Representations](https://arxiv.org/pdf/1802.05365.pdf){:target="_blank"}  
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf){:target="_blank"}  
- [Unified Language Model Pre-training for Natural Language Understanding and Generation](https://arxiv.org/pdf/1905.03197.pdf){:target="_blank"}  
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/pdf/1909.11942.pdf){:target="_blank"}  
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf){:target="_blank"}  
- [ERNIE: Enhanced Language Representation with Informative Entities](https://arxiv.org/pdf/1905.07129.pdf){:target="_blank"}  
- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/pdf/2003.10555.pdf){:target="_blank"}  
  
  
### 3. 대용량 언어 모델  
  
**대용량 언어 모델**
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf){:target="_blank"}  
- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf){:target="_blank"}  
- [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf){:target="_blank"}  
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf){:target="_blank"}  
- [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf){:target="_blank"}  
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf){:target="_blank"}  
- [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf){:target="_blank"}  
- [Mixtral of Experts](https://arxiv.org/pdf/2401.04088.pdf){:target="_blank"}  
  
  
**스케일링 법칙(Scaling Law)**
- [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf){:target="_blank"}  
- [Scaling Laws for Transfer](https://arxiv.org/pdf/2102.01293.pdf){:target="_blank"}  
- [Emergent Abilities of Large Language Models](https://arxiv.org/pdf/2206.07682.pdf){:target="_blank"}  
- [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf){:target="_blank"}  
- [Transcending Scaling Laws with 0.1% Extra Compute](https://arxiv.org/pdf/2210.11399.pdf){:target="_blank"}  
- [Inverse Scaling can become U-shaped](https://arxiv.org/pdf/2211.02011.pdf){:target="_blank"}  
- [Are Emergent Abilities of Large Language Models a Mirage?](https://arxiv.org/pdf/2304.15004.pdf){:target="_blank"}  
   
  
**지시 튜닝(Instruction Tuning) 및 인간 피드백 기반 강화학습(RLHF)**
- [Training Language Models to Follow Instructions with Human Feedback](https://arxiv.org/pdf/2203.02155.pdf){:target="_blank"}  
- [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/pdf/2109.01652.pdf){:target="_blank"}  
- [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207.pdf){:target="_blank"}  
- [LIMA: Less Is More for Alignment](https://arxiv.org/pdf/2305.11206.pdf){:target="_blank"}  
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf){:target="_blank"}  
- [Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/pdf/2310.16944.pdf){:target="_blank"}  
	 					
  
**효율적인 대용량 언어모델 학습**
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf){:target="_blank"}  
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf){:target="_blank"}  
- [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/pdf/2108.12409.pdf){:target="_blank"}  
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/pdf/2104.09864.pdf){:target="_blank"}  
- [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/pdf/1911.02150.pdf){:target="_blank"}  
- [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/pdf/2305.13245v2.pdf){:target="_blank"}  
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf){:target="_blank"}  
						
  
**효율적인 대용량 언어모델 추론**
- [BERT Loses Patience: Fast and Robust Inference with Early Exit](https://arxiv.org/pdf/2006.04152.pdf){:target="_blank"}  
- [Confident Adaptive Language Modeling](https://arxiv.org/pdf/2207.07061.pdf){:target="_blank"}  
- [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/pdf/2211.17192.pdf){:target="_blank"}  
- [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/pdf/2309.06180.pdf){:target="_blank"}  
- [Flash-Decoding for long-context inference](https://pytorch.org/blog/flash-decoding/){:target="_blank"}  
- [Break the Sequential Dependency of LLM Inference Using Lookahead Decoding](https://lmsys.org/blog/2023-11-21-lookahead-decoding/){:target="_blank"}  
						
  
**대용량 언어 모델 압축과 희소화**
- [Efficient Large Scale Language Modeling with Mixtures of Experts](https://arxiv.org/pdf/2112.10684.pdf){:target="_blank"}  
- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/pdf/2006.16668.pdf){:target="_blank"}  
- [CoLT5: Faster Long-Range Transformers with Conditional Computation](https://arxiv.org/pdf/2303.09752.pdf){:target="_blank"}  
- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/pdf/2208.07339.pdf){:target="_blank"}  
- [8-bit Optimizers via Block-wise Quantization](https://arxiv.org/pdf/2110.02861.pdf){:target="_blank"}  
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf){:target="_blank"}  
- [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/pdf/2310.11453.pdf){:target="_blank"}  
						
  
**대용량 언어 모델 프롬프팅(Prompting)**
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf){:target="_blank"}  
- [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/pdf/2203.11171.pdf){:target="_blank"}  
- [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/pdf/2305.10601.pdf){:target="_blank"}  
- [Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks](https://arxiv.org/pdf/2211.12588.pdf){:target="_blank"}  
- [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models](https://arxiv.org/pdf/2205.10625.pdf){:target="_blank"}  
- [Measuring and Narrowing the Compositionality Gap in Language Models](https://arxiv.org/pdf/2210.03350.pdf){:target="_blank"}  
- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629.pdf){:target="_blank"}  
- [Self-Refine: Iterative Refinement with Self-Feedback](https://arxiv.org/pdf/2303.17651.pdf){:target="_blank"}  
  
  
### 4. 멀티모달(Multimodal) 모델
  
**비전 트랜스포머**
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://openreview.net/pdf?id=YicbFdNTTy	){:target="_blank"}  
- [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/pdf/2103.14030.pdf){:target="_blank"}  
- [Training data-efficient image transformers & distillation through attention](https://arxiv.org/pdf/2012.12877.pdf){:target="_blank"}  
- [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/pdf/2104.14294.pdf){:target="_blank"}  
- [SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://arxiv.org/pdf/2303.15446.pdf){:target="_blank"}  
- [MetaFormer Is Actually What You Need for Vision](https://arxiv.org/pdf/2111.11418.pdf){:target="_blank"}  
- [Masked Autoencoders Are Scalable Vision Learners](https://arxiv.org/pdf/2111.06377.pdf){:target="_blank"}  
						
  
**디퓨전(Diffusion) 모델**
- [Maximum Likelihood Training of Score-Based Diffusion Models](https://arxiv.org/pdf/2101.09258.pdf){:target="_blank"}  
- [Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/pdf/2011.13456.pdf){:target="_blank"}  
- [Denoising Diffusion Implicit Models](https://arxiv.org/pdf/2010.02502.pdf){:target="_blank"}  
- [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf){:target="_blank"}  
- [DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps](https://arxiv.org/pdf/2206.00927.pdf){:target="_blank"}  
- [Consistency Models](https://arxiv.org/pdf/2303.01469.pdf){:target="_blank"}  
    
      
**이미지 생성**
- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752.pdf){:target="_blank"}  
- [Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding](https://arxiv.org/pdf/2205.11487.pdf){:target="_blank"}  
- [Scaling Autoregressive Models for Content-Rich Text-to-Image Generation](https://arxiv.org/pdf/2206.10789.pdf){:target="_blank"}  
- [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://arxiv.org/pdf/2204.06125.pdf){:target="_blank"}  
- [PIXART-α: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis](https://arxiv.org/pdf/2310.00426.pdf){:target="_blank"}  
- [Adversarial Diffusion Distillation](https://static1.squarespace.com/static/6213c340453c3f502425776e/t/65663480a92fba51d0e1023f/1701197769659/adversarial_diffusion_distillation.pdf){:target="_blank"}  
  
  
**멀티모달 모델 사전학습**
- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020.pdf){:target="_blank"}  
- [BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/pdf/2201.12086.pdf){:target="_blank"}  
- [CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/pdf/2205.01917.pdf){:target="_blank"}  
- [Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks](https://arxiv.org/pdf/2004.06165.pdf){:target="_blank"}  
- [VinVL: Revisiting Visual Representations in Vision-Language Models](https://arxiv.org/pdf/2101.00529.pdf){:target="_blank"}  
- [Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://arxiv.org/pdf/2102.05918.pdf){:target="_blank"}  
- [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/pdf/2303.15343.pdf){:target="_blank"}  
  
  
**대용량 멀티모달 모델**
- [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/pdf/2204.14198.pdf){:target="_blank"}  
- [Visual Instruction Tuning](https://arxiv.org/pdf/2304.08485.pdf){:target="_blank"}  
- [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/pdf/2305.06500.pdf){:target="_blank"}  
- [PaLI: A Jointly-Scaled Multilingual Language-Image Model](https://arxiv.org/pdf/2209.06794.pdf){:target="_blank"}  
- [PaLI-3 Vision Language Models: Smaller](https://arxiv.org/pdf/2310.09199.pdf){:target="_blank"}  
- [Generative Multimodal Models are In-Context Learners](https://arxiv.org/pdf/2312.13286.pdf){:target="_blank"}  
- [InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks](https://arxiv.org/pdf/2312.14238.pdf){:target="_blank"}  
- [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/pdf/2312.11805v1.pdf){:target="_blank"}  
  
  
### 5. 증강(Augmentation) 기반 모델 
  
**도구(Tool) 증강**
- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/pdf/2302.04761.pdf){:target="_blank"}  
- [ART: Automatic Multi-step Reasoning and Tool-use for Large Language Models](https://arxiv.org/pdf/2303.09014.pdf){:target="_blank"}  
- [ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs](https://arxiv.org/pdf/2307.16789.pdf){:target="_blank"}  
- [ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings](https://arxiv.org/pdf/2305.11554.pdf){:target="_blank"}  
- [AgentBench: Evaluating LLMs as Agents](https://arxiv.org/pdf/2308.03688.pdf){:target="_blank"}  
- [CogAgent: A Visual Language Model for GUI Agents](https://arxiv.org/pdf/2312.08914.pdf){:target="_blank"}  
- [WebArena: A Realistic Web Environment for Building Autonomous Agents](https://arxiv.org/pdf/2307.13854.pdf){:target="_blank"}  
  
  
**검색(Retrieval) 증강**
- [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/pdf/2002.08909.pdf){:target="_blank"}  
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/pdf/2005.11401.pdf){:target="_blank"}  
- [Improving Language Models by Retrieving from Trillions of Tokens](https://arxiv.org/pdf/2112.04426.pdf){:target="_blank"}  
- [Self-RAG: Learning to Retrieve](https://arxiv.org/pdf/2310.11511.pdf){:target="_blank"}  
- [REPLUG: Retrieval-Augmented Black-Box Language Models](https://arxiv.org/pdf/2301.12652.pdf){:target="_blank"}  
    
      
열심히 읽자!  
  
